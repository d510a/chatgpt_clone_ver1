# -*- coding: utf-8 -*-
"""
Streamlit ã‚¢ãƒ—ãƒª: PDF / Word / ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å†…å®¹ã‚’æŠ½å‡ºã—ã¦ OpenAI ChatCompletion
ã«é€ä¿¡ã—ã¾ã™ã€‚
- 400 Bad Request ã‚’é˜²ããŸã‚ã«ãƒˆãƒ¼ã‚¯ãƒ³æ¦‚ç®—ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
- PDF ã¯ pdfminer â†’ PyPDF2 â†’ PyMuPDF â†’ pdfplumber â†’ OCR ã®é †ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
- Word (.docx) ã¯ python-docx ã§æŠ½å‡º
- TXT ã¯ãã®ã¾ã¾èª­ã¿è¾¼ã¿
å¿…è¦ç’°å¢ƒ:
  python -m pip install streamlit openai python-docx pdfminer.six PyPDF2 PyMuPDF pdfplumber pdf2image pytesseract python-dotenv
"""

from __future__ import annotations

import os
import sys
import logging
from io import BytesIO
from pathlib import Path
import re
from typing import Optional, List

import streamlit as st
from dotenv import load_dotenv
import openai
from docx import Document
import pdfplumber  # noqa: pip install pdfplumber

# ==================================================
# ç’°å¢ƒå¤‰æ•° & OpenAI åˆæœŸåŒ–
# --------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ .env ã‚‚ã—ãã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()
openai.api_key = OPENAI_API_KEY

# ===== ãƒ­ã‚°è¨­å®š ====================================
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s: %(message)s",
    stream=sys.stderr,
)

# ===== å®šæ•° ========================================
TOKEN_LIMIT = 7_000           # 1 å›ã® ChatCompletion ã¸ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ¦‚ç®—ï¼‰
CHUNK_MARGIN = 500            # system/assistant ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å›ã™ä½™è£•ãƒˆãƒ¼ã‚¯ãƒ³
POPPLER_DIR = Path(os.getenv("POPPLER_DIR", "/usr/bin"))
TESSERACT_EXE = Path(os.getenv("TESSERACT_EXE", "/usr/bin/tesseract"))

# ===== ã‚¬ãƒ¼ãƒ™ãƒ¼ã‚¸åˆ¤å®šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =================
CID_RE = re.compile(r"\(cid:\d+\)")
REPLACEMENT_CHAR = "\uFFFD"  # 'ï¿½'


def looks_garbled(text: str, threshold: float = 0.20) -> bool:
    """PDF æŠ½å‡ºçµæœãŒæ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹ã‹ç°¡æ˜“åˆ¤å®š"""
    if not text or not text.strip():
        return True
    garbled_tokens = (
        text.count(REPLACEMENT_CHAR)
        + len(CID_RE.findall(text))
    )
    ratio = garbled_tokens / max(len(text), 1)
    return ratio > threshold


def clean_cid(text: str) -> str:
    """(cid:123) ã‚’é™¤å»"""
    return CID_RE.sub("", text)

# ===== PDF æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ =============================

def extract_with_pdfminer(bio: BytesIO) -> Optional[str]:
    try:
        from pdfminer.high_level import extract_text
        text = extract_text(bio)
        return text
    except Exception as e:
        logging.warning("pdfminer å¤±æ•—: %s", e)
        return None


def extract_with_pypdf2(bio: BytesIO) -> Optional[str]:
    try:
        import PyPDF2
        reader = PyPDF2.PdfReader(bio)
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        logging.warning("PyPDF2 å¤±æ•—: %s", e)
        return None


def extract_with_pymupdf(data: bytes) -> Optional[str]:
    try:
        import fitz  # PyMuPDF
        doc = fitz.open(stream=data, filetype="pdf")
        return "\n".join(p.get_text() for p in doc)
    except Exception as e:
        logging.warning("PyMuPDF å¤±æ•—: %s", e)
        return None


def extract_with_pdfplumber(bio: BytesIO) -> Optional[str]:
    try:
        with pdfplumber.open(bio) as pdf:
            text = "\n".join(p.extract_text() or "" for p in pdf.pages)
        return text
    except Exception as e:
        logging.warning("pdfplumber å¤±æ•—: %s", e)
        return None


def ocr_with_tesseract(data: bytes) -> str:
    try:
        from pdf2image import convert_from_bytes
        import pytesseract
        pages = convert_from_bytes(data, dpi=300, fmt="png", poppler_path=str(POPPLER_DIR))
        pytesseract.pytesseract.tesseract_cmd = str(TESSERACT_EXE)
        return "\n".join(pytesseract.image_to_string(p, lang="jpn") for p in pages)
    except Exception as e:
        logging.warning("OCR å¤±æ•—: %s", e)
        return ""


def extract_text_from_pdf(file_obj) -> str:
    data = file_obj.read()
    bio = BytesIO(data)

    for extractor in (
        extract_with_pdfminer,
        extract_with_pypdf2,
        extract_with_pymupdf,
        extract_with_pdfplumber,
    ):
        bio.seek(0)
        text = extractor(bio if extractor != extract_with_pymupdf else data)
        if text and not looks_garbled(text):
            logging.info("%s æˆåŠŸ", extractor.__name__)
            return clean_cid(text)[:180_000]

    logging.info("å…¨ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ‰‹æ³•ãŒå¤±æ•— â†’ OCR ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    ocr_text = ocr_with_tesseract(data)
    if ocr_text.strip():
        return ocr_text[:180_000]
    return "(PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ)"

# ===== Word æŠ½å‡º ====================================

def extract_text_from_docx(file_obj) -> str:
    try:
        file_obj.seek(0)
        doc = Document(BytesIO(file_obj.read()))
        text = "\n".join(p.text for p in doc.paragraphs)
        return text[:180_000] if text else ""
    except Exception as e:
        logging.warning("DOCX å¤±æ•—: %s", e)
        return "(Word ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ)"

# ===== TXT èª­ã¿è¾¼ã¿ =================================

def extract_text_from_txt(file_obj) -> str:
    try:
        return file_obj.read().decode(errors="ignore")[:180_000]
    except Exception:
        file_obj.seek(0)
        return file_obj.read().decode("shift_jis", errors="ignore")[:180_000]

# ===== ãƒˆãƒ¼ã‚¯ãƒ³æ¦‚ç®— & ãƒãƒ£ãƒ³ã‚¯ ======================

def rough_token_len(txt: str) -> int:
    """ãŠãŠã‚ˆã 1 token â‰’ 4 æ–‡å­—ã§æ¦‚ç®—"""
    return max(len(txt) // 4, 1)


def split_into_chunks(txt: str, limit: int = TOKEN_LIMIT) -> List[str]:
    if rough_token_len(txt) <= limit:
        return [txt]
    chunks, buff = [], []
    for line in txt.splitlines():
        buff.append(line)
        if rough_token_len("\n".join(buff)) > (limit - CHUNK_MARGIN):
            chunks.append("\n".join(buff))
            buff = []
    if buff:
        chunks.append("\n".join(buff))
    return chunks

# ===== OpenAI ChatCompletion ãƒ©ãƒƒãƒ‘ ==================

def chat_completion(user_text: str, model: str = "gpt-4o-mini") -> str:
    resp = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {"role": "user", "content": user_text},
        ],
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()

# ==================================================
# Streamlit UI
# --------------------------------------------------

st.set_page_config(page_title="ãƒ•ã‚¡ã‚¤ãƒ« ChatGPT", layout="wide")
st.title("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ ChatGPT ã§è§£æ")

with st.sidebar:
    st.header("è¨­å®š")
    model = st.selectbox("ãƒ¢ãƒ‡ãƒ«", ["gpt-4o-mini", "gpt-4o", "gpt-4-turbo"])  # ãŠå¥½ã¿ã§è¿½åŠ 
    st.markdown("---")
    st.caption("ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆã¯è‡ªå‹•ã§ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã™ã€‚")

uploaded = st.file_uploader(
    "PDF / Word / ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ",
    type=["pdf", "docx", "txt"],
)

if uploaded:
    suffix = Path(uploaded.name).suffix.lower()
    with st.spinner("ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­ â€¦"):
        if suffix == ".pdf":
            extracted = extract_text_from_pdf(uploaded)
        elif suffix == ".docx":
            extracted = extract_text_from_docx(uploaded)
        else:
            extracted = extract_text_from_txt(uploaded)

    if not extracted or extracted.startswith("("):
        st.error("ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.stop()

    chunks = split_into_chunks(extracted)
    st.success(f"{len(chunks)} ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦é€ä¿¡ã—ã¾ã™ â€¦")

    for i, ck in enumerate(chunks, 1):
        with st.spinner(f"ChatGPT ({i}/{len(chunks)}) å‡¦ç†ä¸­ â€¦"):
            response = chat_completion(ck, model=model)
        st.subheader(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {i}")
        st.write(response)

    st.download_button(
        "æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=extracted,
        file_name=f"{Path(uploaded.name).stem}_extracted.txt",
        mime="text/plain",
    )
